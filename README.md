# GenAI Security Research Project

## Overview

This repository contains proof-of-concept code for GenAI security research, including Large Language Model (LLM) security research and Multi-agent Chat Protocol (MCP) security experiments. The codebase demonstrates various security vulnerabilities, attack vectors, and defense mechanisms related to AI systems.


## Resources & References

- [PINT Benchmark](https://github.com/lakeraai/pint-benchmark) - Benchmark for testing LLM security vulnerabilities
- [Who is Gandalf?](https://www.lakera.ai/blog/who-is-gandalf) - Lakera's analysis of the Gandalf LLM security challenge
- [Security Research](https://github.com/harishsg993010) - Additional security research resources
- [Deep Dive MCP and A2A Attack Vectors for AI Agents](https://www.solo.io/blog/deep-dive-mcp-and-a2a-attack-vectors-for-ai-agents) - Explore critical security vulnerabilities in AI agent ecosystems
- [MCP Security Notification: Tool Poisoning Attacks](https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks) - Security notification regarding tool poisoning attacks in MCP
- [WhatsApp MCP Exploited: Exfiltrating your message history via MCP](https://invariantlabs.ai/blog/whatsapp-mcp-exploited) - Demonstrates how an untrusted MCP server can attack and exfiltrate data from an agentic system
- [Offensive ML Playbook](https://wiki.offsecml.com/)

## Security Notice
⚠️ IMPORTANT: This code is for educational and research purposes only. Some implementations demonstrate security vulnerabilities and should not be used in production environments without proper safeguards.